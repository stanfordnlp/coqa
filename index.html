<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>CoQA: A Conversational Question Answering Challenge</title><meta name="description" content="CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" name="og:image" content="/logo.png"><meta property="twitter:image" name="twitter:image" content="/logo.png"><link rel="image_src" type="image/png" href="/coqa/logo.png"><link rel="shortcut icon" href="/coqa/favicon.ico" type="image/x-icon"><link rel="icon" href="/coqa/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/coqa/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/coqa/stylesheets/layout.css"><link rel="stylesheet" href="/coqa/stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/coqa/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/coqa/">Home</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/coqa/">CoQA</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle"><img src="logo.png" width="10%" style="visibility:hidden"> CoQA <img src="logo.png" width="15%"></h1><h2 id="appSubtitle">A Conversational Question Answering Challenge</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is CoQA?</h2></div><p> CoQA is a large-scale dataset for building <span><b>Co</b>nversational <b>Q</b>uestion <b>A</b>nswering </span>systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. CoQA is pronounced as <a href="https://en.wikipedia.org/wiki/Coca">coca <img src="logo.png" width="8%"></a>.</p><a class="btn actionBtn" href="http://arxiv.org/abs/1808.07042">CoQA paper</a><hr><p><b> CoQA </b>contains 127,000+ questions with answers collected from 8000+ conversations. 
Each conversation  
is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. 
The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains.
CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.</p><div class="infoHeadline"><h2>Download</h2></div><p>Browse the examples in CoQA:<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://drive.google.com/open?id=1ik0d_nIsGdXLn8o7tYiiDWN6PK2XNy-D">Browse CoQA</a></li></ul></p><p> Download a copy of the dataset in json format:<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json" download>Download Training Set (47 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json" download>Download Dev Set (9 MB)</a></li></ul></p><hr><div class="infoHeadline"><h2>Evaluation</h2></div><p>To evaluate your models, use the official evaluation script. To run the evaluation, use <code>python evaluate-v1.0.py --data-file &lt;path_to_dev-v1.0.json&gt; --pred-file &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://nlp.stanford.edu/data/coqa/evaluate-v1.0.py" download>Evaluation Script</a></li><li><a class="btn actionBtn inverseBtn" href="https://nlp.stanford.edu/data/coqa/drqa-pgnet-coqa-dev-hist1.txt.json" download>Sample Prediction File (on Dev Set)</a></li></ul></p><p>Once you are satisfied with your model performance on the dev set, you submit it to get the official scores on the test sets. We have two test sets, an in-domain set which constitutes the domains present in the training and the dev sets, and an out-of-domain set which constitutes unseen domains (see the paper for more details). To preserve the integrity of the test results, we do not release the test set to the public. Follow this tutorial on how to submit your model for an official evaluation:</p><a class="btn actionBtn inverseBtn" href="https://github.com/stanfordnlp/coqa-baselines/blob/master/codalab.md">Submission Tutorial</a><div class="infoHeadline"><h2>License</h2></div><p>CoQA contains passages from seven domains. We make five of these public under the following licenses:<ul class="list"><li>Literature and Wikipedia passages are shared under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 </a>license. </li><li>Children's stories are collected from <a href="https://www.microsoft.com/en-us/research/publication/mctest-challenge-dataset-open-domain-machine-comprehension-text/">MCTest </a>which comes with <a href="https://github.com/mcobzarenco/mctest/blob/master/data/MCTest/LICENSE.pdf">MSR-LA </a>license. </li><li>Middle/High school exam passages are collected from <a href="https://arxiv.org/abs/1704.04683">RACE </a>which comes with its <a href="http://www.cs.cmu.edu/~glai1/data/race/">own </a>license. </li><li>News passages are collected from the <a href="https://arxiv.org/abs/1506.03340">DeepMind CNN dataset  </a>which comes with <a href="https://github.com/deepmind/rc-data/blob/master/LICENSE">Apache </a>license. </li></ul></p><div class="infoHeadline"><h2>Questions?</h2></div><p> Ask us questions at our <a href="https://groups.google.com/forum/#!forum/coqa">google group </a>or at <a href="mailto:sivar@cs.stanford.edu">sivar@cs.stanford.edu </a>or <a href="mailto:danqi@cs.stanford.edu">danqi@cs.stanford.edu</a>.</p><div class="infoHeadline"><h2>Acknowledgements</h2></div><p> We thank the <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD team </a>for allowing us to use their code and templates for generating this website.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanfordnlp.github.com/coqa" data-text="CoQA: A Conversational Question Answering Challenge  -- 127,000 questions from 8000 conversations on text passages" data-via="stanfordnlp" data-size="large" data-hashtags="CoQA">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><table class="table performanceTable"><tr><th>Rank</th><th>Model</th><th>In-domain</th><th>Out-of-domain</th><th>Overall</th></tr><tr class="human-row"><td></td><td>Human Performance<p class="institution">Stanford University</p><a href="http://arxiv.org/abs/1808.07042">(Reddy et al. '18)</a></td><td>89.4</td><td>87.4</td><td>88.8</td></tr><tr><td> <p>1</p><span class="date label label-default">Oct 06, 2018</span></td><td style="word-break:break-word;">FlowQA (single model)<p class="institution">Allen Institute for Artificial Intelligence</p><a class="link" href="https://arxiv.org/abs/1810.06683">https://arxiv.org/abs/1810.06683</a></td><td><b>76.3</b></td><td><b>71.8</b></td><td><b>75.0</b></td></tr><tr><td> <p>2</p><span class="date label label-default">Sep 27, 2018</span></td><td style="word-break:break-word;">BiDAF++ (single model)<p class="institution">Allen Institute for Artificial Intelligence</p><a class="link" href="https://arxiv.org/abs/1809.10735">https://arxiv.org/abs/1809.10735</a></td><td>69.4</td><td>63.8</td><td>67.8</td></tr><tr><td> <p>3</p><span class="date label label-default">Aug 21, 2018</span></td><td style="word-break:break-word;">DrQA + seq2seq with copy attention (single model)<p class="institution">Stanford University</p><a class="link" href="https://arxiv.org/abs/1808.07042">https://arxiv.org/abs/1808.07042</a></td><td>67.0</td><td>60.4</td><td>65.1</td></tr><tr><td> <p>4</p><span class="date label label-default">Aug 21, 2018</span></td><td style="word-break:break-word;">Vanilla DrQA (single model)<p class="institution">Stanford University</p><a class="link" href="https://arxiv.org/abs/1808.07042">https://arxiv.org/abs/1808.07042</a></td><td>54.5</td><td>47.9</td><td>52.6</td></tr></table></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="rightNav"><div><ul class="nav navbar-nav navbar-right"><li><a href="/coqa/">CoQA</a></li><li><a href="http://nlp.stanford.edu">Stanford NLP Group</a></li></ul></div></div></div></nav><script src="/coqa/bower_components/jquery/dist/jquery.min.js"></script><script src="/coqa/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>