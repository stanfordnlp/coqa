extends layout

block title
  title A Conversational Question Answering Challenge

block description
  meta(name='description', content='CoQA is a large-scale dataset for building Conversational Question Answering systems.')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin squad_2_model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th In-domain
      th Out-of-domain
      th Overall
    - var human_in_domain_f1 = 89.387
    - var human_out_of_domain_f1 = 87.374
    - var human_overall_f1 = 88.798
    - var largest_in_domain_f1 = Math.max.apply(null, group.map(function (model) { return model.in_domain_f1; }))
    - var largest_out_of_domain_f1 = Math.max.apply(null, group.map(function (model) { return model.out_of_domain_f1; }))
    - var largest_overall_f1 = Math.max.apply(null, group.map(function (model) { return model.overall_f1; }))
      tr.human-row
        td
        td
          | Human Performance
          p.institution Stanford University
          a(href="http://arxiv.org/abs/1606.05250") (Reddy et al. '18)
        td #{human_in_domain_f1}
        td #{human_out_of_domain_f1}
        td #{human_overall_f1}
    each model in group
      tr
        if is_test
          td 
            p #{model.rank}
            span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
        td(style="word-break:break-word;")
          | #{model.model_name}
          p.institution #{model.institution}
          if model.link
            a.link(href=model.link) #{model.link}
        td
          if model.in_domain_f1 == largest_in_domain_f1
            b #{model.in_domain_f1.toPrecision(5)}
          else
            | #{model.in_domain_f1.toPrecision(5)}
        td
          if model.out_of_domain_f1 == largest_out_of_domain_f1
            b #{model.out_of_domain_f1.toPrecision(5)}
          else
            | #{model.out_of_domain_f1.toPrecision(5)}
        td
          if model.overall_f1 == largest_overall_f1
            b #{model.overall_f1.toPrecision(5)}
          else
            | #{model.overall_f1.toPrecision(5)}


block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is CoQA?
              p 
                | CoQA is a large-scale dataset for building 
                span
                  b Co
                  |nversational 
                  b Q
                  |uestion 
                  b A
                  |nswering 
                | systems. The goal of CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. CoQA is prounounced as 
                a(href="https://en.wikipedia.org/wiki/Coca") coca <img src="logo.png" width="8%">.
              hr
              p
                b  CoQA 
                | contains 127,000+ questions with answers collected from 8000+ conversations. 
                | Each conversation in 
                | CoQA is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. 
                | The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also contains an evidence highlighted in the passage; and 4) the passages are collected from seven diverse domains.
                | CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.

              //-a.btn.actionBtn(href="/explore/1.1/dev/") Explore SQuAD and model predictions
              a.btn.actionBtn(href="http://arxiv.org/abs/TODO") CoQA paper (Reddy et al. '18)
              .infoHeadline
                h2 Download
              p
                | Before downloading the dataset, we assume you have the licenses mentioned 
                a(href="https://github.com/stanfordnlp/coqa#license") at this url
                | .
              hr
              p
                | To browse conversations in CoQA, click below.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://drive.google.com/open?id=1ik0d_nIsGdXLn8o7tYiiDWN6PK2XNy-D")
                      | Browse Dev Set

                  li
                    a.btn.actionBtn.inverseBtn(href="https://drive.google.com/open?id=1wekA7RU9eZSGItBudr58NTYZUCukYZih")
                      | Browse Training Set
              p 
                | To download the dataset in json format, click below.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0xe254829ab81946198433c4da847fb485/contents/blob/", download)
                      | Download Dev Set (9 MB)
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0xe3674fd34560425786f97541ec91aeb8/contents/blob/", download)
                      | Download Training Set (47 MB)
              hr
              p To evaluate your models, please use the official evaluation script. To run the evaluation, use 
                code
                  | python evaluate-v1.0.py --data-file &lt;path_to_dev-v1.0.json&gt; --pred-file &lt;path_to_predictions&gt;
                |.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0x3f956c8b9eec44588dbe536d38eb7fb9/contents/blob/", download)
                      | Evaluation Script
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0xd584cdc127284880a65554a849d09afc/contents/blob/", download)
                      | Sample Prediction File (on Dev Set)
              p Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:
              a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/worksheets/0x374c0149636344479ba93db1f4814fbb/")
                | Submission Tutorial
              .infoHeadline
                h2 Have Questions?
              p 
                | If you have a question about the challenge, please raise an issue on our 
                a(href="https://github.com/stanfordnlp/coqa") github page
                | .
            .infoSubheadline
              include includes/tweet
              include includes/github
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              +squad_2_model_display(test2, true)
